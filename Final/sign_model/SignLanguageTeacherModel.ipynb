{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b439cbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.1.2\n",
      "Uninstalling torch-2.1.2:\n",
      "  Successfully uninstalled torch-2.1.2\n",
      "Found existing installation: transformers 4.39.3\n",
      "Uninstalling transformers-4.39.3:\n",
      "  Successfully uninstalled transformers-4.39.3\n",
      "Found existing installation: pillow 10.2.0\n",
      "Uninstalling pillow-10.2.0:\n",
      "  Successfully uninstalled pillow-10.2.0\n",
      "Found existing installation: opencv-python 4.8.1.78\n",
      "Uninstalling opencv-python-4.8.1.78:\n",
      "  Successfully uninstalled opencv-python-4.8.1.78\n",
      "Found existing installation: mediapipe 0.10.7\n",
      "Uninstalling mediapipe-0.10.7:\n",
      "  Successfully uninstalled mediapipe-0.10.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall -y torch transformers pillow opencv-python mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e088357c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.1.2\n",
      "  Using cached torch-2.1.2-cp310-cp310-win_amd64.whl.metadata (26 kB)\n",
      "Collecting transformers==4.39.3\n",
      "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "Collecting pillow==10.2.0\n",
      "  Using cached pillow-10.2.0-cp310-cp310-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting opencv-python==4.8.1.78\n",
      "  Using cached opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting mediapipe==0.10.7\n",
      "  Using cached mediapipe-0.10.7-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torch==2.1.2) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\adity\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.1.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\adity\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.1.2) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torch==2.1.2) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torch==2.1.2) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\adity\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.1.2) (2025.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\adity\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.39.3) (0.32.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\adity\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.39.3) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adity\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.39.3) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers==4.39.3) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers==4.39.3) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\adity\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.39.3) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\adity\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.39.3) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\adity\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.39.3) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers==4.39.3) (4.64.1)\n",
      "Requirement already satisfied: absl-py in c:\\users\\adity\\appdata\\roaming\\python\\python310\\site-packages (from mediapipe==0.10.7) (2.2.2)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from mediapipe==0.10.7) (22.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\adity\\appdata\\roaming\\python\\python310\\site-packages (from mediapipe==0.10.7) (25.2.10)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\adity\\anaconda3\\lib\\site-packages (from mediapipe==0.10.7) (3.7.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\adity\\appdata\\roaming\\python\\python310\\site-packages (from mediapipe==0.10.7) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\adity\\appdata\\roaming\\python\\python310\\site-packages (from mediapipe==0.10.7) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\adity\\appdata\\roaming\\python\\python310\\site-packages (from mediapipe==0.10.7) (0.5.2)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe==0.10.7) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\adity\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.7) (2.21)\n",
      "Requirement already satisfied: colorama in c:\\users\\adity\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.39.3) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from jinja2->torch==2.1.2) (2.1.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe==0.10.7) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe==0.10.7) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe==0.10.7) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe==0.10.7) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe==0.10.7) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe==0.10.7) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.7) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->transformers==4.39.3) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->transformers==4.39.3) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->transformers==4.39.3) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->transformers==4.39.3) (2022.12.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from sympy->torch==2.1.2) (1.2.1)\n",
      "Using cached torch-2.1.2-cp310-cp310-win_amd64.whl (192.3 MB)\n",
      "Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "Using cached pillow-10.2.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "Using cached opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl (38.1 MB)\n",
      "Using cached mediapipe-0.10.7-cp310-cp310-win_amd64.whl (50.3 MB)\n",
      "Installing collected packages: pillow, opencv-python, torch, mediapipe, transformers\n",
      "\n",
      "   ---------------------------------------- 0/5 [pillow]\n",
      "   ---------------------------------------- 0/5 [pillow]\n",
      "   ---------------------------------------- 0/5 [pillow]\n",
      "   -------- ------------------------------- 1/5 [opencv-python]\n",
      "   -------- ------------------------------- 1/5 [opencv-python]\n",
      "   -------- ------------------------------- 1/5 [opencv-python]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [torch]\n",
      "   ------------------------ --------------- 3/5 [mediapipe]\n",
      "   ------------------------ --------------- 3/5 [mediapipe]\n",
      "   ------------------------ --------------- 3/5 [mediapipe]\n",
      "   ------------------------ --------------- 3/5 [mediapipe]\n",
      "   ------------------------ --------------- 3/5 [mediapipe]\n",
      "   ------------------------ --------------- 3/5 [mediapipe]\n",
      "   ------------------------ --------------- 3/5 [mediapipe]\n",
      "   ------------------------ --------------- 3/5 [mediapipe]\n",
      "   ------------------------ --------------- 3/5 [mediapipe]\n",
      "   ------------------------ --------------- 3/5 [mediapipe]\n",
      "   ------------------------ --------------- 3/5 [mediapipe]\n",
      "   ------------------------ --------------- 3/5 [mediapipe]\n",
      "   ------------------------ --------------- 3/5 [mediapipe]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   ---------------------------------------- 5/5 [transformers]\n",
      "\n",
      "Successfully installed mediapipe-0.10.7 opencv-python-4.8.1.78 pillow-10.2.0 torch-2.1.2 transformers-4.39.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.1.2 \\\n",
    "            transformers==4.39.3 \\\n",
    "            pillow==10.2.0 \\\n",
    "            opencv-python==4.8.1.78 \\\n",
    "            mediapipe==0.10.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b01e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "# from transformers import AutoImageProcessor, SiglipForImageClassification\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the classification model\n",
    "# model_name = \"prithivMLmods/Alphabet-Sign-Language-Detection\"\n",
    "# model = SiglipForImageClassification.from_pretrained(model_name)\n",
    "# processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# # Label map\n",
    "# labels = {\n",
    "#     \"0\": \"A\", \"1\": \"B\", \"2\": \"C\", \"3\": \"D\", \"4\": \"E\", \"5\": \"F\", \"6\": \"G\", \"7\": \"H\", \"8\": \"I\", \"9\": \"J\",\n",
    "#     \"10\": \"K\", \"11\": \"L\", \"12\": \"M\", \"13\": \"N\", \"14\": \"O\", \"15\": \"P\", \"16\": \"Q\", \"17\": \"R\", \"18\": \"S\", \"19\": \"T\",\n",
    "#     \"20\": \"U\", \"21\": \"V\", \"22\": \"W\", \"23\": \"X\", \"24\": \"Y\", \"25\": \"Z\"\n",
    "# }\n",
    "\n",
    "# # Mediapipe Hands setup\n",
    "# mp_hands = mp.solutions.hands\n",
    "# hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
    "# mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# # Start webcam\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# while True:\n",
    "#     success, frame = cap.read()\n",
    "#     if not success:\n",
    "#         break\n",
    "\n",
    "#     # Flip frame and convert to RGB\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "#     img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Process hand detection\n",
    "#     result = hands.process(img_rgb)\n",
    "\n",
    "#     if result.multi_hand_landmarks:\n",
    "#         for hand_landmarks in result.multi_hand_landmarks:\n",
    "#             # Get bounding box of hand\n",
    "#             h, w, _ = frame.shape\n",
    "#             x_coords = [lm.x * w for lm in hand_landmarks.landmark]\n",
    "#             y_coords = [lm.y * h for lm in hand_landmarks.landmark]\n",
    "#             x1, y1, x2, y2 = int(min(x_coords)), int(min(y_coords)), int(max(x_coords)), int(max(y_coords))\n",
    "\n",
    "#             # Padding and bounds\n",
    "#             padding = 20\n",
    "#             x1, y1 = max(0, x1 - padding), max(0, y1 - padding)\n",
    "#             x2, y2 = min(w, x2 + padding), min(h, y2 + padding)\n",
    "\n",
    "#             # Crop hand ROI\n",
    "#             hand_roi = frame[y1:y2, x1:x2]\n",
    "#             if hand_roi.size == 0:\n",
    "#                 continue\n",
    "\n",
    "#             # Convert to PIL and classify\n",
    "#             image_pil = Image.fromarray(cv2.cvtColor(hand_roi, cv2.COLOR_BGR2RGB)).convert(\"RGB\")\n",
    "#             inputs = processor(images=image_pil, return_tensors=\"pt\")\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = model(**inputs)\n",
    "#                 probs = torch.nn.functional.softmax(outputs.logits, dim=1).squeeze().tolist()\n",
    "\n",
    "#             pred_class = labels[str(np.argmax(probs))]\n",
    "#             confidence = max(probs)\n",
    "\n",
    "#             # Draw box and prediction\n",
    "#             cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "#             cv2.putText(frame, f'{pred_class} ({confidence:.2f})', (x1, y1 - 10),\n",
    "#                         cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "#             # Draw landmarks\n",
    "#             mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "#     cv2.imshow(\"Sign Detection\", frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3507dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from transformers import AutoImageProcessor, SiglipForImageClassification\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the classification model\n",
    "model_name = \"prithivMLmods/Alphabet-Sign-Language-Detection\"\n",
    "model = SiglipForImageClassification.from_pretrained(model_name)\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Label map\n",
    "labels = {\n",
    "    \"0\": \"A\", \"1\": \"B\", \"2\": \"C\", \"3\": \"D\", \"4\": \"E\", \"5\": \"F\", \"6\": \"G\", \"7\": \"H\", \"8\": \"I\", \"9\": \"J\",\n",
    "    \"10\": \"K\", \"11\": \"L\", \"12\": \"M\", \"13\": \"N\", \"14\": \"O\", \"15\": \"P\", \"16\": \"Q\", \"17\": \"R\", \"18\": \"S\", \"19\": \"T\",\n",
    "    \"20\": \"U\", \"21\": \"V\", \"22\": \"W\", \"23\": \"X\", \"24\": \"Y\", \"25\": \"Z\"\n",
    "}\n",
    "\n",
    "# Mediapipe Hands setup\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Your original classifier\n",
    "def sign_language_classification(image):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1).squeeze().tolist()\n",
    "\n",
    "    labels = {\n",
    "        str(i): chr(65 + i) for i in range(26)  # \"0\" to \"25\" mapped to A-Z\n",
    "    }\n",
    "    predictions = {labels[str(i)]: round(probs[i], 3) for i in range(len(probs))}\n",
    "    return predictions\n",
    "\n",
    "# Webcam + detection\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(img_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            h, w, _ = frame.shape\n",
    "            x_coords = [lm.x * w for lm in hand_landmarks.landmark]\n",
    "            y_coords = [lm.y * h for lm in hand_landmarks.landmark]\n",
    "            x1, y1, x2, y2 = int(min(x_coords)), int(min(y_coords)), int(max(x_coords)), int(max(y_coords))\n",
    "\n",
    "            padding = 20\n",
    "            x1, y1 = max(0, x1 - padding), max(0, y1 - padding)\n",
    "            x2, y2 = min(w, x2 + padding), min(h, y2 + padding)\n",
    "\n",
    "            hand_roi = frame[y1:y2, x1:x2]\n",
    "            if hand_roi.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Use your trusted classifier\n",
    "            hsv = cv2.cvtColor(hand_roi, cv2.COLOR_BGR2HSV)\n",
    "            # Step 2: Equalize only the V (brightness) channel\n",
    "            hsv[..., 2] = cv2.equalizeHist(hsv[..., 2])\n",
    "            # Step 3: Convert back to BGR color space\n",
    "            enhanced_bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "            # Step 4 (Optional): Slight contrast/brightness tuning\n",
    "            alpha, beta = 1.2, 15  # Increase contrast and brightness\n",
    "            final = cv2.convertScaleAbs(enhanced_bgr, alpha=alpha, beta=beta)\n",
    "            # Step 5: Convert to PIL for classification\n",
    "            image_pil = Image.fromarray(cv2.cvtColor(final, cv2.COLOR_BGR2RGB)).convert(\"RGB\")\n",
    "            predictions = sign_language_classification(image_pil)\n",
    "\n",
    "            # Get top prediction\n",
    "            top_pred = max(predictions.items(), key=lambda x: x[1])\n",
    "            pred_class, confidence = top_pred\n",
    "\n",
    "            # Draw results\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'{pred_class} ({confidence:.2f})', (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    cv2.imshow(\"Sign Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94c1d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"export/sign_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4459d2e-4042-415e-b5a0-9a0597358d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"export/sign_model/config.json\", \"r\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46948b4c-7c43-4784-8bef-84d553c4cd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['export/sign_model\\\\preprocessor_config.json']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.save_pretrained(\"export/sign_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3097e4a9-a915-4f34-a87b-bb29cc2fd16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocessing.py\n",
    "import torch\n",
    "\n",
    "labels = {str(i): chr(65+i) for i in range(26)}\n",
    "\n",
    "def postprocess(logits):\n",
    "    probs = torch.nn.functional.softmax(logits, dim=1).squeeze().tolist()\n",
    "    return {labels[str(i)]: round(probs[i], 3) for i in range(len(probs))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05267832-1b7e-4bf1-a028-8046b6d99379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_hand_roi(roi):\n",
    "    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "    hsv[..., 2] = cv2.equalizeHist(hsv[..., 2])\n",
    "    enhanced = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    final = cv2.convertScaleAbs(enhanced, alpha=1.2, beta=15)\n",
    "    return Image.fromarray(cv2.cvtColor(final, cv2.COLOR_BGR2RGB)).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "837b36b8-65e4-4793-823f-9e84cfcbc56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"export/sign_model/model_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf5be019-ed1d-4fae-91a5-583cb5c5e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "label_map = {str(i): chr(65+i) for i in range(26)}\n",
    "with open(\"export/label_map.json\", \"w\") as f:\n",
    "    json.dump(label_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c8504-542d-40c1-8f0b-66c863bad24f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
