{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redd-hope/HumanPatterns/blob/main/StudentModelMaking_smm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3YWMV9t9gAke"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import ViTModel\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)\n",
        "        self.norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.attn(x, x, x)\n",
        "        return self.norm(x + attn_output)\n",
        "\n",
        "class HybridDeepModel(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super().__init__()\n",
        "        # Vit output: [B, N+1, 768]\n",
        "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "        # CNN to compress to 256\n",
        "        self.cnn1 = nn.Sequential(\n",
        "            nn.Conv1d(768, 512, 3, padding=1),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(512, 256, 3, padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # CNN again after first transitions\n",
        "        self.cnn2 = nn.Sequential(\n",
        "            nn.Conv1d(256, 256, 3, padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.bi_lstm = nn.LSTM(256, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
        "\n",
        "        self.attn_256 = lambda: SelfAttention(256)\n",
        "\n",
        "        # All attention layers\n",
        "        self.attn1 = self.attn_256()\n",
        "        self.attn2 = self.attn_256()\n",
        "        self.attn3 = self.attn_256()\n",
        "        self.attn4 = self.attn_256()\n",
        "        self.attn5 = self.attn_256()\n",
        "        self.attn6 = self.attn_256()\n",
        "\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # === ViT\n",
        "        vit_out = self.vit(pixel_values=x).last_hidden_state[:, 1:, :]  # [B, N-1, 768]\n",
        "\n",
        "        # === CNN in parallel\n",
        "        cnn_in = vit_out.transpose(1, 2)\n",
        "        cnn_out = self.cnn1(cnn_in).transpose(1, 2)  # [B, N-1, 256]\n",
        "\n",
        "        # === Parallel Merge: (ViT || CNN) → Attention\n",
        "        combined = cnn_out + vit_out[:, :, :256]  # reduce ViT dim to 256 for merge\n",
        "        out = self.attn1(combined)\n",
        "\n",
        "        # === CNN → Attention\n",
        "        out = out.transpose(1, 2)\n",
        "        out = self.cnn2(out).transpose(1, 2)\n",
        "        out = self.attn2(out)\n",
        "\n",
        "        # === BiLSTM → Attention\n",
        "        out, _ = self.bi_lstm(out)\n",
        "        out = self.attn3(out)\n",
        "\n",
        "        # === (CNN || BiLSTM) → Attention\n",
        "        cnn_branch = self.cnn2(out.transpose(1, 2)).transpose(1, 2)\n",
        "        lstm_branch, _ = self.bi_lstm(out)\n",
        "        out = cnn_branch + lstm_branch\n",
        "        out = self.attn4(out)\n",
        "\n",
        "        # === (ViT || BiLSTM) → Attention\n",
        "        vit_compress = vit_out[:, :, :256]\n",
        "        lstm_branch2, _ = self.bi_lstm(out)\n",
        "        out = vit_compress + lstm_branch2\n",
        "        out = self.attn5(out)\n",
        "\n",
        "        # === Final BiLSTM → Attention\n",
        "        out, _ = self.bi_lstm(out)\n",
        "        out = self.attn6(out)\n",
        "\n",
        "        # === Classification\n",
        "        logits = self.classifier(out)  # [B, seq_len, num_classes]\n",
        "\n",
        "        return logits.permute(1, 0, 2)  # CTC format: [T, B, C]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_XVMEJI4LUZ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}